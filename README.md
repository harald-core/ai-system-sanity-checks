# AI System Sanity Checks

Most AI systems do not fail catastrophically.
They degrade quietly.

This repository documents a small set of **conceptual sanity checks**
for AI-driven systems at the **architecture level**.

This is intentionally **non-implementational**.

---

## What this repository is

- A collection of system-level failure patterns
- A vocabulary for discussing AI system stability
- A thinking aid for architects and system designers

## What this repository is NOT

- Not a framework
- Not a library
- Not production code
- Not a prompt collection
- Not an implementation guide

---

## Why this exists

Many failures attributed to "bad models"
are actually **system design failures**.

This repository focuses on those failure modes,
without providing directly reusable implementations.
